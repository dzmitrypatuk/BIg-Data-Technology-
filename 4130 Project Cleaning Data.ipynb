{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb9ee6bd-5f62-4cfd-aa8c-6cba76b1981a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#%pip install \"boto3>=1.28\" \"s3fs>=2023.3.0\"\n",
    "\n",
    "import boto3\n",
    "import time\n",
    "import os\n",
    "import seaborn as sns\n",
    "import pyarrow\n",
    "\n",
    "from pyspark.sql.functions import col, isnan, when, count, udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "adc89b93-6ddf-4ada-8448-3841fee91baf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# To work with Amazon S3 storage, set the following variables using your AWS Access Key and Secret Key\n",
    "# Set the Region to where your files are stored in S3.\n",
    "access_key = ''\n",
    "secret_key = ''\n",
    "# Set the environment variables so boto3 can pick them up later\n",
    "os.environ['AWS_ACCESS_KEY_ID'] = access_key\n",
    "os.environ['AWS_SECRET_ACCESS_KEY'] = secret_key\n",
    "encoded_secret_key = secret_key.replace(\"/\", \"%2F\").replace(\"+\", \"%2B\")\n",
    "aws_region = \"us-east-2\"\n",
    "# Set this to the name of your bucket where the files are stored\n",
    "aws_bucket_name = \"amazon-reviews-project-dp\"\n",
    "mount_name = \"s3dataread\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65e4aede-9f38-49c5-bc32-ce6f89eb6479",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", access_key) \n",
    "\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", secret_key) \n",
    "\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"s3.\" + aws_region + \".amazonaws.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07c0062b-718e-426e-9a5b-f4016d7ab9fe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# The Amazon S3 bucket can be mounted like a local directory using the Databricks dbfs file system\n",
    "# Mount Amazon S3 Bucket as a local file system\n",
    "#dbutils.fs.mount(f\"s3a://{access_key}:{encoded_secret_key}@{aws_bucket_name}\", f\"/mnt/{mount_name}\")\n",
    "#display(dbutils.fs.ls(f\"/mnt/{mount_name}\"))\n",
    "#file_location = \"dbfs:/mnt/s3dataread/landing/*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92317a1f-3a40-4337-beba-018e99775f1f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#function to clean data and prepare dataframe for the 'raw' folder  \n",
    "def perform_EDA(sdf, filename):\n",
    "    print(f\"{filename} Number of records:{sdf.count()}\" )\n",
    "    print('Number of records: ', reviews_sdf.count())\n",
    "    reviews_sdf.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in [\"star_rating\", \"review_body\"]] ).show()\n",
    "\n",
    "def perform_cleaning(sdf, filename, out_filepath):\n",
    "    sdf.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in [\"star_rating\", \"review_body\"]] ).show()\n",
    "    sdf = sdf.na.drop(subset=[\"star_rating\", \"review_body\"])\n",
    "    # Turn this function into a User-Defined Function (UDF)\n",
    "    ascii_udf = udf(ascii_only)\n",
    "    # Clean up the review_headline and review_body\n",
    "    sdf = sdf.withColumn(\"clean_review_headline\", ascii_udf('review_headline'))\n",
    "    sdf = sdf.withColumn(\"clean_review_body\", ascii_udf('review_body'))\n",
    "    # Re-check the cleaned headline and body\n",
    "    sdf.select(\"clean_review_headline\", \"clean_review_body\").summary(\"count\", \"min\", \"max\").show()\n",
    "    sdf.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in [\"star_rating\", \"review_body\"]] ).show()\n",
    "\n",
    "    # Save the cleaned dataframe as Parquet\n",
    "    #output_file_path=\"s3://amazon-reviews-project-dp/raw/cleaned_amazon_reviews_us_Apparel_v1_00.parquet\"\n",
    "    output_file_path= f\"{out_filepath}cleaned_{filename}.parquet\"\n",
    "    #output_file_path= f\"{filepath}{filename}\"\n",
    "    sdf.write.parquet(output_file_path) \n",
    "\n",
    "\n",
    "# Define a function to strip out any non-ascii character\n",
    "def ascii_only(mystring):\n",
    "    if mystring:\n",
    "        return mystring.encode('ascii', 'ignore').decode('ascii')\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b65a16a0-66bc-4705-90a7-16f1ef415cea",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on file: amazon_reviews_us_Apparel_v1_00.tsv\nroot\n |-- marketplace: string (nullable = true)\n |-- customer_id: integer (nullable = true)\n |-- review_id: string (nullable = true)\n |-- product_id: string (nullable = true)\n |-- product_parent: integer (nullable = true)\n |-- product_title: string (nullable = true)\n |-- product_category: string (nullable = true)\n |-- star_rating: integer (nullable = true)\n |-- helpful_votes: integer (nullable = true)\n |-- total_votes: integer (nullable = true)\n |-- vine: string (nullable = true)\n |-- verified_purchase: string (nullable = true)\n |-- review_headline: string (nullable = true)\n |-- review_body: string (nullable = true)\n |-- review_date: date (nullable = true)\n\n+-----------+-----------+\n|star_rating|review_body|\n+-----------+-----------+\n|         11|       1060|\n+-----------+-----------+\n\n+-------+---------------------+--------------------+\n|summary|clean_review_headline|   clean_review_body|\n+-------+---------------------+--------------------+\n|  count|              5905272|             5905273|\n|    min|                     |                    |\n|    max|              WOW!!!|Makes me feel li...|\n+-------+---------------------+--------------------+\n\n+-----------+-----------+\n|star_rating|review_body|\n+-----------+-----------+\n|          0|          0|\n+-----------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Amazon Product Reviews\n",
    "# Path to Amazon S3 files\n",
    "filepath = \"s3://amazon-reviews-project-dp/landing/\"\n",
    "out_filepath = \"s3://amazon-reviews-project-dp/raw/\"\n",
    "\n",
    "# List of data files\n",
    "filename_list = [ 'amazon_reviews_us_Apparel_v1_00.tsv']\n",
    "\n",
    "#Read files from the list of files names in the s3 bucket\n",
    "for filename in filename_list:\n",
    "    # Read in amazon reviews. Reminder: Tab-separated values files\n",
    "    print(f\"Working on file: {filename}\")\n",
    "    reviews_sdf = spark.read.csv(f\"{filepath}{filename}\", sep='\\t', header=True, inferSchema=True)\n",
    "    reviews_sdf.printSchema()\n",
    "    #perform_EDA(reviews_df,filename)\n",
    "    perform_cleaning(reviews_sdf, filename, out_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5beca78a-1d23-46c7-8b8d-566211e6778a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c3f5c1f-6c04-48a3-af1d-2add2d270c4a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce53c213-ae5a-4b92-a761-f1492e1e2233",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4535c88-9c17-47ef-96a6-88697a8ea72d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "4130 Project: Cleaning Data",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
