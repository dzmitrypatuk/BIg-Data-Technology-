{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4da613e7-406f-47ed-a222-89d47217049c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install \"boto3>=1.28\" \"s3fs>=2023.3.0\"\n",
    "%pip install -U textblob\n",
    "#%%python -m textblob.download_corpora\n",
    "\n",
    "import boto3\n",
    "import time\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "import pyarrow\n",
    "\n",
    "from pyspark.sql.functions import col, isnan, isnull, when, count, udf, size, split, year, month, format_number, date_format, length\n",
    "from pyspark.sql.types import IntegerType, DateType, StringType, StructType, DoubleType\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.feature import FeatureHasher\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, OneHotEncoder, Normalizer, StandardScaler, HashingTF, IDF, Tokenizer, RegexTokenizer\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression, LogisticRegressionModel\n",
    "from pyspark.ml.regression import GeneralizedLinearRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "from pyspark.ml.linalg import DenseMatrix, Vectors\n",
    "from pyspark.ml.stat import Correlation\n",
    "\n",
    "#from textblob import TextBlob\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Import the logistic regression model\n",
    "# Import the evaluation module\n",
    "from pyspark.ml.evaluation import *\n",
    "# Import the model tuning module\n",
    "from pyspark.ml.tuning import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "286ee4b2-fc57-4ab9-9571-a37f0c4182ec",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# To work with Amazon S3 storage, set the following variables using your AWS Access Key and Secret Key\n",
    "# Set the Region to where your files are stored in S3.\n",
    "access_key = ''\n",
    "secret_key = ''\n",
    "# Set the environment variables so boto3 can pick them up later\n",
    "os.environ['AWS_ACCESS_KEY_ID'] = access_key\n",
    "os.environ['AWS_SECRET_ACCESS_KEY'] = secret_key\n",
    "encoded_secret_key = secret_key.replace(\"/\", \"%2F\").replace(\"+\", \"%2B\")\n",
    "aws_region = \"us-east-2\"\n",
    "# Set this to the name of your bucket where the files are stored\n",
    "aws_bucket_name = \"amazon-reviews-project-dp\"\n",
    "mount_name = \"s3dataread\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae3c478f-3030-453f-b195-673e4b0af4b9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", access_key) \n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", secret_key) \n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"s3.\" + aws_region + \".amazonaws.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61a10c5b-e781-4bea-9b64-465b036e4726",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Amazon Product Reviews\n",
    "# Path to Amazon S3 files\n",
    "filepath = \"s3://amazon-reviews-project-dp/raw/\"\n",
    "out_filepath = \"s3://amazon-reviews-project-dp/trusted/\"\n",
    "\n",
    "# List of data files\n",
    "filename_list = ['cleaned_amazon_reviews_us_Apparel_v1_00.tsv.parquet']\n",
    "\n",
    "#Read files from the list of files names in the s3 bucket\n",
    "for filename in filename_list:\n",
    "    # Read in amazon reviews. Reminder: Tab-separated values files\n",
    "    print(f\"Working on file: {filename}\")\n",
    "    reviews_sdf = spark.read.parquet(f\"{filepath}{filename}\")\n",
    "    reviews_sdf.printSchema()\n",
    "    reviews_sdf.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8fc710e1-cb1b-4d0d-81f5-f0c44832f0c0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "selected_sdf = reviews_sdf.select(['star_rating', 'total_votes', 'review_body', 'vine', 'product_category', 'verified_purchase', 'helpful_votes', 'clean_review_headline', 'clean_review_body']).sample(False, 0.1, 42)\n",
    "\n",
    "\n",
    "# Create a count of the review words\n",
    "selected_sdf = selected_sdf.withColumn('review_body_wordcount', size(split(col('review_body'), ' ')))\n",
    "\n",
    "selected_sdf = selected_sdf.withColumn(\"total_votes\",selected_sdf.total_votes.cast(DoubleType()))\n",
    "selected_sdf = selected_sdf.withColumn(\"helpful_votes\",selected_sdf.total_votes.cast(DoubleType()))\n",
    "selected_sdf = selected_sdf.withColumn(\"review_body_wordcount\",selected_sdf.review_body_wordcount.cast(DoubleType()))\n",
    "\n",
    "# Create a label. =1 if over 3, =0 if otherwise\n",
    "selected_sdf = selected_sdf.withColumn(\"label\", when(selected_sdf.star_rating >= 4, 1.0).otherwise(0.0) )\n",
    "\n",
    "# tokenizer = Tokenizer(inputCol=\"clean_review_body\", outputCol=\"clean_review_words\")\n",
    "tokenizer = RegexTokenizer(inputCol=\"clean_review_body\", outputCol=\"clean_review_words\", pattern=\"\\\\w+\", gaps=False)\n",
    "selected_sdf = tokenizer.transform(selected_sdf)\n",
    "\n",
    "# Run the hash function over the tokens\n",
    "selected_sdf = selected_sdf.drop('clean_review_tf')\n",
    "hashtf = HashingTF(numFeatures=2**13, inputCol=\"clean_review_words\", outputCol='clean_review_tf')\n",
    "selected_sdf = hashtf.transform(selected_sdf)\n",
    "\n",
    "idf = IDF(inputCol='clean_review_tf', outputCol=\"clean_review_features\", minDocFreq=5)\n",
    "selected_sdf = idf.fit(selected_sdf).transform(selected_sdf)\n",
    "\n",
    "# Create an indexer for the string based columns.\n",
    "indexer = StringIndexer(inputCols=[\"product_category\", \"vine\", \"verified_purchase\"], outputCols=[\"product_categoryIndex\", \"vineIndex\", \"verified_purchaseIndex\"], handleInvalid=\"keep\")\n",
    "\n",
    "# Create an encoder for the indexes\n",
    "encoder = OneHotEncoder(inputCols=[\"product_categoryIndex\", \"vineIndex\", \"verified_purchaseIndex\" ],\n",
    "                        outputCols=[\"product_categoryVector\", \"vineVector\", \"verified_purchaseVector\" ], dropLast=True, handleInvalid=\"keep\")\n",
    "\n",
    "# Assemble all of the vectors together into one large vector named \"features\"\n",
    "# Inlcude the vector from the TF/IDF \"clean_review_features\"\n",
    "assembler = VectorAssembler(inputCols=[\"product_categoryVector\", \"vineVector\", \"verified_purchaseVector\", \"total_votes\", \"review_body_wordcount\", \"clean_review_features\"], outputCol=\"features\")\n",
    "\n",
    "# Build the pipeline with all of the stages for indexer, encoder, toekenizer etc.\n",
    "reviews_pipe = Pipeline(stages=[indexer, encoder, assembler])\n",
    "\n",
    "# Call .fit to transform the data\n",
    "transformed_sdf = reviews_pipe.fit(selected_sdf).transform(selected_sdf)\n",
    "\n",
    "transformed_sdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dba5f2cc-a2fd-4b18-808e-00bfe453a698",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Split the data into 70% training and 30% test sets\n",
    "trainingData, testData = transformed_sdf.randomSplit([0.7, 0.3], seed=42)\n",
    "# Create a LogisticRegression Estimator\n",
    "lr = LogisticRegression()\n",
    "# Fit the model to the training data\n",
    "model = lr.fit(trainingData)\n",
    "# Show model coefficients and intercept\n",
    "print(\"Coefficients: \", model.coefficients)\n",
    "print(\"Intercept: \", model.intercept)\n",
    "# Test the model on the testData\n",
    "test_results = model.transform(testData)\n",
    "# Show the test results\n",
    "test_results.select('product_category', 'vine', 'verified_purchase', 'clean_review_body', 'rawPrediction','probability','prediction',\n",
    "'label').show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff4f0582-0c6a-41a8-b1fe-6a0897df4d33",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save the confusion matrix\n",
    "cm = test_results.groupby('label').pivot('prediction').count().fillna(0).collect()\n",
    "def calculate_recall_precision(cm):\n",
    "    tn = cm[0][1] # True Negative\n",
    "    fp = cm[0][2] # False Positive\n",
    "    fn = cm[1][1] # False Negative\n",
    "    tp = cm[1][2] # True Positive\n",
    "    precision = tp / ( tp + fp )\n",
    "    recall = tp / ( tp + fn )\n",
    "    accuracy = ( tp + tn ) / ( tp + tn + fp + fn )\n",
    "    f1_score = 2 * ( ( precision * recall ) / ( precision + recall ) )\n",
    "    return accuracy, precision, recall, f1_score\n",
    "print( calculate_recall_precision(cm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc5b4a1c-3337-4ee3-96de-cd09058b746d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a BinaryClassificationEvaluator to evaluate how well the model works\n",
    "evaluator = BinaryClassificationEvaluator(metricName=\"areaUnderROC\")\n",
    "# Create the parameter grid (empty for now)\n",
    "grid = ParamGridBuilder().build()\n",
    "# Create the CrossValidator\n",
    "cv = CrossValidator(estimator=lr, estimatorParamMaps=grid, evaluator=evaluator, numFolds=3 )\n",
    "# Use the CrossValidator to Fit the training data\n",
    "cv = cv.fit(trainingData)\n",
    "# Show the average performance over the three folds\n",
    "cv.avgMetrics\n",
    "# Evaluate the test data using the cross-validator model\n",
    "# Reminder: We used Area Under the Curve\n",
    "evaluator.evaluate(cv.transform(testData))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d705fbe5-8ff1-4942-aaa6-3fc54bed2a8e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cv.avgMetrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9473c8fa-6655-4656-bfb3-1b935ed69776",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Test the model on the testData\n",
    "test_results = model.transform(testData)\n",
    "# Confusion matrix on the test_results for Logistic regression\n",
    "test_results.groupby('label').pivot('prediction').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "094c9460-605d-4b0d-b71b-64efff124628",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a grid to hold hyperparameters\n",
    "grid = ParamGridBuilder()\n",
    "grid = grid.addGrid(lr.regParam, [0.2, 0.4, 0.6, 0.8] )\n",
    "grid = grid.addGrid(lr.elasticNetParam, [0, 1])\n",
    "# Build the grid\n",
    "grid = grid.build()\n",
    "print('Number of models to be tested: ', len(grid))\n",
    "# Create the CrossValidator using the new hyperparameter grid\n",
    "cv1 = CrossValidator(estimator=lr, estimatorParamMaps=grid, evaluator=evaluator)\n",
    "# Call cv.fit() to create models with all of the combinations of parameters in the grid\n",
    "all_models = cv1.fit(trainingData)\n",
    "print(\"Average Metrics for Each model: \", all_models.avgMetrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03ad55fb-b7f4-41e3-8130-daf47ebb193f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Gather the metrics and parameters of the model with the best average metrics\n",
    "hyperparams = all_models.getEstimatorParamMaps()[np.argmax(all_models.avgMetrics)]\n",
    "# Print out the list of hyperparameters for the best model\n",
    "for i in range(len(hyperparams.items())):\n",
    "    print([x for x in hyperparams.items()][i])\n",
    "#(Param(parent='LogisticRegression_2effdf339a6c', name='regParam', doc='regularization parameter (>= 0).'), 0.4)\n",
    "#(Param(parent='LogisticRegression_2effdf339a6c', name='elasticNetParam', doc='the ElasticNet mixing parameter, in\n",
    "#range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'), 0.0)\n",
    "# Choose the best model\n",
    "bestModel = all_models.bestModel\n",
    "print(\"Area under ROC curve:\", bestModel.summary.areaUnderROC)\n",
    "# Area under ROC curve: 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "658d7bad-0f3c-477b-a88a-527c4918eb21",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Use the model 'bestModel' to predict the test set\n",
    "test_results = bestModel.transform(testData)\n",
    "# Show the results\n",
    "test_results.select('product_category', 'vine', 'verified_purchase', 'clean_review_body', 'rawPrediction','probability','prediction',\n",
    "'label').show(truncate=False)\n",
    "# Evaluate the predictions. Area Under ROC curve\n",
    "print(evaluator.evaluate(test_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8624f3ab-6408-4fa4-93cf-ce544ac68f91",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Save the trusted dataframe as Parquet\n",
    "output_file_path= f\"{out_filepath}trusted_{filename}\"\n",
    "#output_file_path= f\"{filepath}{filename}\"\n",
    "transformed_sdf.write.parquet(output_file_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "571e0cd9-3722-4135-a95d-8f85b877a1e1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#model path\n",
    "model_path = \"s3://amazon-reviews-project-dp/models/\"\n",
    "\n",
    "# Save the best model\n",
    "model_name = \"amazon_reviews_logistic_regression_model\"\n",
    "model.write().overwrite().save(f\"{model_path}{model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f2b1b5b1-dd5f-4fb0-aad7-f7d080e8349a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "### Code parts \n",
    "    # Filter out short review body texts\n",
    "    #selected_sdf = selected_sdf.where(length(selected_sdf.clean_review_body) > 10)\n",
    "\n",
    "    # Filter out review body texts with 5 or fewer words\n",
    "    #selected_sdf = selected_sdf.where(selected_sdf.review_body_wordcount > 5)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "4130 Project: Feature Engineering and Model Training",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
